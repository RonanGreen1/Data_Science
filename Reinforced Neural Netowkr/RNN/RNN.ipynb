{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Import gym to create the CartPole environment\n",
    "import gym\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Check the observation and action spaces to understand the environment\n",
    "print(\"Observation space:\", env.observation_space)  # 4 continuous values (state)\n",
    "print(\"Action space:\", env.action_space)            # 2 discrete actions: 0 (left), 1 (right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`import gym`**:  \n",
    "    Imports the **OpenAI Gym** library, which provides standard environments for reinforcement learning experiments.\n",
    "    \n",
    "- **`env = gym.make('CartPole-v1')`**:  \n",
    "    Creates an instance of the **CartPole-v1** environment where the agent will attempt to balance a pole on a moving cart.\n",
    "    \n",
    "- **`print(\"Observation space:\", env.observation_space)`**:  \n",
    "    Displays the range and shape of the observation space.\n",
    "    \n",
    "    - The observation space consists of **4 continuous values**:\n",
    "        \n",
    "        - Cart position (within ±4.8 units)\n",
    "            \n",
    "        - Cart velocity (extremely large range indicated by `±3.4028235e+38`)\n",
    "            \n",
    "        - Pole angle (approximately ±0.418)\n",
    "            \n",
    "        - Pole angular velocity (also extremely large range due to simulation limits)\n",
    "            \n",
    "- **`print(\"Action space:\", env.action_space)`**:  \n",
    "    Displays the action space.\n",
    "    \n",
    "    - The environment has **2 discrete actions**:\n",
    "        \n",
    "        - `0` = push cart to the left\n",
    "            \n",
    "        - `1` = push cart to the right\n",
    "            \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- It is critical to understand the environment’s **input (state)** and **output (action)** spaces before building a neural network.\n",
    "    \n",
    "- The **number of input features** (4) determines the input layer size.\n",
    "    \n",
    "- The **number of actions** (2) determines the number of outputs in the final layer of the Q-network.\n",
    "    \n",
    "- Understanding value ranges helps normalisation if needed and informs learning dynamics.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- The environment’s state consists of **4 continuous values**:\n",
    "    \n",
    "    - cart position, cart velocity, pole angle, and pole angular velocity.\n",
    "        \n",
    "- The environment allows **2 discrete actions**: move left (`0`) or right (`1`).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ronan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m50\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import TensorFlow and Keras layers for building the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Build the Q-network model\n",
    "# Input: 4 features from the environment\n",
    "# Output: 2 Q-values (one for each possible action)\n",
    "model = Sequential([\n",
    "    # First hidden layer with 24 neurons and ReLU activation\n",
    "    Dense(24, input_shape=(4,), activation='relu'),\n",
    "    \n",
    "    # Second hidden layer with 24 neurons and ReLU activation\n",
    "    Dense(24, activation='relu'),\n",
    "    \n",
    "    # Output layer with 2 neurons (Q-values for each action)\n",
    "    Dense(2, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model with mean squared error loss (for predicting Q-values)\n",
    "# and Adam optimizer (adaptive, stable convergence)\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "# Display model summary to verify structure\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`import tensorflow as tf` and layers**:  \n",
    "    Imports TensorFlow and the necessary components from Keras for building the neural network.\n",
    "    \n",
    "- **`model = Sequential([...])`**:  \n",
    "    Creates a **sequential neural network**, meaning layers are added one after the other.\n",
    "    \n",
    "- **`Dense(24, input_shape=(4,), activation='relu')`**:  \n",
    "    The first fully connected (dense) layer with:\n",
    "    \n",
    "    - 24 neurons\n",
    "        \n",
    "    - `input_shape=(4,)` means it expects 4 input features (cart position, cart velocity, pole angle, pole angular velocity)\n",
    "        \n",
    "    - `relu` activation introduces non-linearity so the network can learn complex patterns.\n",
    "        \n",
    "- **Second Dense layer**:\n",
    "    \n",
    "    - Another hidden layer with 24 neurons and `relu` activation, allowing the network to learn more complex relationships between inputs and Q-values.\n",
    "        \n",
    "- **Output Dense layer**:\n",
    "    \n",
    "    - 2 neurons (each representing the Q-value for one of the two actions: move left or move right)\n",
    "        \n",
    "    - Uses `linear` activation (default), as Q-values are continuous numbers without bounds.\n",
    "        \n",
    "- **`model.compile(loss='mse', optimizer=Adam(...))`**:\n",
    "    \n",
    "    - **Loss**: mean squared error (MSE) is used because we are predicting continuous values (Q-values) and minimising the difference between predicted and target values.\n",
    "        \n",
    "    - **Adam optimizer**: an adaptive gradient-based optimisation method that generally converges faster and more stably than alternatives.\n",
    "        \n",
    "    - Learning rate is set to **0.001**, a standard starting point.\n",
    "        \n",
    "- **`model.summary()`**:  \n",
    "    Displays the structure and parameter counts for each layer.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- I need a function approximator (Q-network) that takes the environment state and predicts Q-values for both actions.\n",
    "    \n",
    "- Having two hidden layers of reasonable size (24 neurons) gives enough complexity to capture the patterns without making the model too large or prone to overfitting.\n",
    "    \n",
    "- Using **MSE loss** and **Adam** helps the model learn stable and accurate predictions of Q-values over many episodes.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- **120 parameters in the first layer**:\n",
    "    \n",
    "    - `(4 input features + 1 bias) * 24 neurons = 120`\n",
    "        \n",
    "- **600 parameters in the second layer**:\n",
    "    \n",
    "    - `(24 input neurons + 1 bias) * 24 neurons = 600`\n",
    "        \n",
    "- **50 parameters in the output layer**:\n",
    "    \n",
    "    - `(24 input neurons + 1 bias) * 2 output neurons = 50`\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay memory created with capacity: 2000\n"
     ]
    }
   ],
   "source": [
    "# Import deque for creating a memory buffer\n",
    "from collections import deque\n",
    "\n",
    "# Set up replay memory to store past experiences (state, action, reward, next_state, done)\n",
    "# This allows the agent to learn from past experiences and break correlation between steps\n",
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "print(\"Replay memory created with capacity:\", replay_memory.maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`from collections import deque`**  \n",
    "    Imports the `deque` data structure from Python’s `collections` module.\n",
    "    \n",
    "    - A `deque` (double-ended queue) allows for efficient addition and removal of elements from both ends.\n",
    "        \n",
    "- **`replay_memory = deque(maxlen=2000)`**  \n",
    "    Creates a **replay memory buffer** with a maximum length of **2000 experiences**.\n",
    "    \n",
    "    - Each experience will later be stored as a tuple: `(state, action, reward, next_state, done)`\n",
    "        \n",
    "    - Once the buffer reaches 2000 entries, the oldest experiences are automatically removed to make room for new ones.\n",
    "        \n",
    "- **`print(\"Replay memory created with capacity:\", replay_memory.maxlen)`**  \n",
    "    Prints out the capacity of the replay memory to confirm that it has been created correctly.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- The **replay memory** is crucial for Deep Q-Learning.\n",
    "    \n",
    "- Instead of learning from consecutive steps, the model will randomly sample experiences from this buffer.\n",
    "    \n",
    "- This random sampling breaks correlations, stabilises learning, and allows the neural network to generalise better.\n",
    "    \n",
    "- Using a fixed-size `deque` makes memory management automatic and efficient.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- Confirms that the replay memory is set up and ready to store up to 2000 past experiences.\n",
    "    \n",
    "- This buffer will be filled during training as the agent interacts with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters set:\n",
      "gamma=0.95, epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995, batch_size=64, episodes=500\n"
     ]
    }
   ],
   "source": [
    "# Set key hyperparameters for training\n",
    "\n",
    "# Discount factor (gamma) — determines how much future rewards are valued\n",
    "gamma = 0.95  \n",
    "\n",
    "# Initial exploration rate (epsilon) — probability of choosing a random action\n",
    "epsilon = 1.0  \n",
    "\n",
    "# Minimum exploration rate — ensures some exploration continues throughout training\n",
    "epsilon_min = 0.01  \n",
    "\n",
    "# Decay rate for epsilon — reduces exploration over time as the agent learns\n",
    "epsilon_decay = 0.995  \n",
    "\n",
    "# Batch size for training from replay memory\n",
    "batch_size = 64  \n",
    "\n",
    "# Number of episodes (full runs of the environment) to train over\n",
    "episodes = 500  \n",
    "\n",
    "print(\"Hyperparameters set:\")\n",
    "print(f\"gamma={gamma}, epsilon_start={epsilon}, epsilon_min={epsilon_min}, epsilon_decay={epsilon_decay}, batch_size={batch_size}, episodes={episodes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`gamma = 0.95`**\n",
    "    \n",
    "    - The **discount factor**.\n",
    "        \n",
    "    - It determines how much future rewards are valued compared to immediate rewards.\n",
    "        \n",
    "    - A gamma of **0.95** means the agent values future rewards highly but slightly less than immediate rewards.\n",
    "        \n",
    "    - A higher gamma (close to 1) makes the agent plan long-term, while a lower gamma makes it short-sighted.\n",
    "        \n",
    "- **`epsilon = 1.0`**\n",
    "    \n",
    "    - The **initial exploration rate**.\n",
    "        \n",
    "    - This means the agent will start by taking actions completely at random (100%) to explore the environment.\n",
    "        \n",
    "    - High exploration at the start is important to avoid biasing toward early random experiences.\n",
    "        \n",
    "- **`epsilon_min = 0.01`**\n",
    "    \n",
    "    - The **minimum exploration rate**.\n",
    "        \n",
    "    - Even after training for a while, the agent will still take random actions 1% of the time to avoid becoming too rigid and getting stuck in local optima.\n",
    "        \n",
    "- **`epsilon_decay = 0.995`**\n",
    "    \n",
    "    - The rate at which epsilon gradually decreases after each episode.\n",
    "        \n",
    "    - A decay factor of **0.995** means that after each episode, epsilon is multiplied by 0.995, slowly moving from 1.0 down to 0.01.\n",
    "        \n",
    "    - This allows the agent to shift from exploration (random moves) to exploitation (taking the best-known actions).\n",
    "        \n",
    "- **`batch_size = 64`**\n",
    "    \n",
    "    - When training from the replay memory, the agent will randomly sample 64 experiences at a time to update the model.\n",
    "        \n",
    "    - This balances training stability and speed; smaller batches train faster but can be less stable, larger batches slow training down.\n",
    "        \n",
    "- **`episodes = 500`**\n",
    "    \n",
    "    - The number of complete runs (episodes) of the environment the agent will train for.\n",
    "        \n",
    "    - Each episode runs until the pole falls or the time limit is reached.\n",
    "        \n",
    "    - 500 is a good number for CartPole — long enough for the agent to converge to good behaviour.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- Hyperparameters control how the agent learns.\n",
    "    \n",
    "- They balance exploration vs. exploitation, determine long-term vs. short-term rewards, and affect learning speed and stability.\n",
    "    \n",
    "- Without carefully chosen hyperparameters, the agent can:\n",
    "    \n",
    "    - Fail to learn (too little exploration or short-sighted gamma).\n",
    "        \n",
    "    - Overexplore and not exploit (high epsilon without decay).\n",
    "        \n",
    "    - Become stuck or slow (poor batch sizes).\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- Confirms that all learning parameters are correctly set.\n",
    "    \n",
    "- The agent will start exploring randomly, slowly learn to exploit actions that lead to high rewards, and value future rewards while training in batches of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to choose an action based on the current state\n",
    "def choose_action(state, epsilon):\n",
    "    # With probability epsilon, take a random action\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # Predict the Q-values for the state and choose the action with the highest Q-value (exploitation)\n",
    "        q_values = model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])  # Return the action with the highest predicted Q-value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`def choose_action(state, epsilon):`**  \n",
    "    Defines a function that takes in the current environment state and the current exploration rate `epsilon`.\n",
    "    \n",
    "- **`if np.random.rand() <= epsilon:`**\n",
    "    \n",
    "    - Generates a random number between 0 and 1.\n",
    "        \n",
    "    - If it’s less than or equal to `epsilon`, the agent takes a **random action** — this is exploration.\n",
    "        \n",
    "    - High `epsilon` means more frequent random moves at the beginning.\n",
    "        \n",
    "- **`env.action_space.sample()`**\n",
    "    \n",
    "    - This randomly picks an action from the environment's action space.\n",
    "        \n",
    "    - In CartPole, this is either **0 (move left)** or **1 (move right)**.\n",
    "        \n",
    "- **`else:`**\n",
    "    \n",
    "    - If the random number is greater than `epsilon`, the agent uses the model to predict the best action — exploitation.\n",
    "        \n",
    "- **`q_values = model.predict(state, verbose=0)`**\n",
    "    \n",
    "    - The Q-network predicts the expected reward (Q-values) for both actions given the current state.\n",
    "        \n",
    "    - Output will be an array with two values: one for action 0, one for action 1.\n",
    "        \n",
    "- **`return np.argmax(q_values[0])`**\n",
    "    \n",
    "    - The action corresponding to the highest Q-value is chosen, meaning the model selects the action it believes will lead to the best future reward.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- The agent needs to balance **exploration** (trying new actions) and **exploitation** (choosing the best-known action).\n",
    "    \n",
    "- This function applies the **epsilon-greedy strategy**, the standard method for balancing these two.\n",
    "    \n",
    "- By wrapping it in a function, the main loop stays clean, and it’s easy to tweak or replace the action-selection logic if needed.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model using random samples from the replay memory\n",
    "def replay(batch_size):\n",
    "    # Don't train until there is enough experiences to fill a batch\n",
    "    if len(replay_memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Randomly sample a batch of experiences from the memory\n",
    "    minibatch = np.random.choice(len(replay_memory), batch_size, replace=False)\n",
    "    \n",
    "    # Prepare lists for states and target Q-values\n",
    "    states = []\n",
    "    targets = []\n",
    "    \n",
    "    # Process each sampled experience\n",
    "    for index in minibatch:\n",
    "        state, action, reward, next_state, done = replay_memory[index]\n",
    "        \n",
    "        # Predict current Q-values for the state\n",
    "        target = model.predict(state, verbose=0)[0]\n",
    "        \n",
    "        # If the episode is done, set Q-value for that action to reward only\n",
    "        if done:\n",
    "            target[action] = reward\n",
    "        else:\n",
    "            # Predict Q-values for the next state and use max Q-value for target update\n",
    "            t = model.predict(next_state, verbose=0)[0]\n",
    "            target[action] = reward + gamma * np.amax(t)\n",
    "        \n",
    "        states.append(state[0])\n",
    "        targets.append(target)\n",
    "    \n",
    "    # Train the model on the entire batch in one go\n",
    "    model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- `def replay(batch_size):`  \n",
    "    This function takes a `batch_size` and trains the model by using random experiences from the replay memory.\n",
    "    \n",
    "- `if len(replay_memory) < batch_size: return`  \n",
    "    It first checks if there are enough experiences in memory. If not, it returns.\n",
    "    \n",
    "- `minibatch = np.random.choice(len(replay_memory), batch_size, replace=False)`  \n",
    "    Randomly selects a set of indices from the replay memory without repetition, creating a mini-batch of experiences to learn from.\n",
    "    \n",
    "- `states = []` and `targets = []`  \n",
    "    These empty lists will store input states and the corresponding target Q-values for training.\n",
    "    \n",
    "- `for index in minibatch:`  \n",
    "    Iterates over each selected experience.\n",
    "    \n",
    "    - `state, action, reward, next_state, done = replay_memory[index]` extracts each experience tuple.\n",
    "        \n",
    "    - `target = model.predict(state, verbose=0)[0]` gets the current Q-values for the given state.\n",
    "        \n",
    "    - If the episode has ended (`done` is `True`), the Q-value for that action is set to the immediate reward.\n",
    "        \n",
    "    - Otherwise, it adds the reward plus the discounted future reward, calculated as `reward + gamma * max predicted Q-value of the next state`.\n",
    "        \n",
    "    - The state and calculated target Q-values are stored for batch training.\n",
    "        \n",
    "- `model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)`  \n",
    "    After processing the entire batch, the model is trained on these states and updated Q-values all at once for one epoch.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- This function allows the neural network to learn from many past experiences, not just the most recent action.\n",
    "    \n",
    "- It helps **break the correlation** between consecutive experiences by training on random samples.\n",
    "    \n",
    "- This stabilises learning and prevents the model from overfitting to patterns in sequential data.\n",
    "    \n",
    "- The logic for updating Q-values follows the **Bellman Equation**, using the reward and the estimated best future action.\n",
    "    \n",
    "- By training on batches, we allow more stable, generalised learning rather than on single data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]  # Reset environment at start of episode\n",
    "    state = np.reshape(state, [1, 4])  # Reshape for model input\n",
    "    total_reward = 0  # Track total reward for the episode\n",
    "    \n",
    "    for step in range(500):  # Limit steps to avoid infinite loops\n",
    "        # Choose action based on current state and epsilon\n",
    "        action = choose_action(state, epsilon)\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        \n",
    "        # Store experience in replay memory\n",
    "        replay_memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Train the network from memory\n",
    "        replay(batch_size)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    # Print progress every 50 episodes\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode: {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- `for episode in range(episodes):`  \n",
    "    Starts the main training loop, running for a set number of episodes. Each episode represents one full run of the environment until the pole falls or the time limit is reached.\n",
    "    \n",
    "- `state = env.reset()[0]`  \n",
    "    Resets the environment to its starting conditions and returns the initial state.  \n",
    "    The `[0]` index is used because `env.reset()` returns a tuple (state, info).\n",
    "    \n",
    "- `state = np.reshape(state, [1, 4])`  \n",
    "    Reshapes the state array to the correct shape for feeding into the neural network (1 row, 4 features).\n",
    "    \n",
    "- `total_reward = 0`  \n",
    "    Initialises a counter to track the total reward earned during the episode.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "- `for step in range(500):`  \n",
    "    Limits each episode to 500 steps to avoid infinite runs if the agent manages to balance the pole for a long time.\n",
    "    \n",
    "- `action = choose_action(state, epsilon)`  \n",
    "    Uses the previously defined epsilon-greedy function to choose the next action.\n",
    "    \n",
    "    - Either a random action (exploration) or the model’s predicted best action (exploitation).\n",
    "        \n",
    "- `next_state, reward, done, _, _ = env.step(action)`  \n",
    "    Executes the chosen action in the environment.\n",
    "    \n",
    "    - `next_state`: the new environment state.\n",
    "        \n",
    "    - `reward`: the reward given for the action.\n",
    "        \n",
    "    - `done`: `True` if the episode has ended.\n",
    "        \n",
    "    - `_`: placeholders for values returned but not used.\n",
    "        \n",
    "- `next_state = np.reshape(next_state, [1, 4])`  \n",
    "    Reshapes the next state to be compatible with the model input format.\n",
    "    \n",
    "- `replay_memory.append((state, action, reward, next_state, done))`  \n",
    "    Stores the experience in the replay buffer for future training.\n",
    "    \n",
    "- `state = next_state`  \n",
    "    Updates the current state for the next step.\n",
    "    \n",
    "- `total_reward += reward`  \n",
    "    Adds the reward from this step to the running total.\n",
    "    \n",
    "- `replay(batch_size)`  \n",
    "    Calls the replay function to train the model on a random batch of past experiences.\n",
    "    \n",
    "- `if done: break`  \n",
    "    Ends the episode early if the environment signals that the pole has fallen or time limit hit.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "- `if epsilon > epsilon_min: epsilon *= epsilon_decay`  \n",
    "    Reduces the value of epsilon after each episode, gradually shifting from exploration to exploitation.\n",
    "    \n",
    "- `if (episode + 1) % 50 == 0:`  \n",
    "    Every 50 episodes, prints out the progress:\n",
    "    \n",
    "    - Episode number\n",
    "        \n",
    "    - Total reward earned in that episode\n",
    "        \n",
    "    - Current value of epsilon (exploration rate).\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- This loop is the core of reinforcement learning, where the agent:\n",
    "    \n",
    "    - Observes the state\n",
    "        \n",
    "    - Takes actions\n",
    "        \n",
    "    - Collects rewards\n",
    "        \n",
    "    - Stores experience\n",
    "        \n",
    "    - Learns from past experiences via replay\n",
    "        \n",
    "    - Gradually becomes more confident (reducing exploration with epsilon decay)\n",
    "        \n",
    "- Without this loop, the neural network would not learn how to maximise the reward or improve its Q-value predictions.\n",
    "    \n",
    "- Limiting episode length and decaying epsilon ensures efficient, stable training and avoids endless exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
