{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Import gym to create the CartPole environment\n",
    "import gym\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Check the observation and action spaces to understand the environment\n",
    "print(\"Observation space:\", env.observation_space)  # 4 continuous values (state)\n",
    "print(\"Action space:\", env.action_space)            # 2 discrete actions: 0 (left), 1 (right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`import gym`**:  \n",
    "    Imports the **OpenAI Gym** library, which provides standard environments for reinforcement learning experiments.\n",
    "    \n",
    "- **`env = gym.make('CartPole-v1')`**:  \n",
    "    Creates an instance of the **CartPole-v1** environment where the agent will attempt to balance a pole on a moving cart.\n",
    "    \n",
    "- **`print(\"Observation space:\", env.observation_space)`**:  \n",
    "    Displays the range and shape of the observation space.\n",
    "    \n",
    "    - The observation space consists of **4 continuous values**:\n",
    "        \n",
    "        - Cart position (within ±4.8 units)\n",
    "            \n",
    "        - Cart velocity (extremely large range indicated by `±3.4028235e+38`)\n",
    "            \n",
    "        - Pole angle (approximately ±0.418)\n",
    "            \n",
    "        - Pole angular velocity (also extremely large range due to simulation limits)\n",
    "            \n",
    "- **`print(\"Action space:\", env.action_space)`**:  \n",
    "    Displays the action space.\n",
    "    \n",
    "    - The environment has **2 discrete actions**:\n",
    "        \n",
    "        - `0` = push cart to the left\n",
    "            \n",
    "        - `1` = push cart to the right\n",
    "            \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- It is critical to understand the environment’s **input (state)** and **output (action)** spaces before building a neural network.\n",
    "    \n",
    "- The **number of input features** (4) determines the input layer size.\n",
    "    \n",
    "- The **number of actions** (2) determines the number of outputs in the final layer of the Q-network.\n",
    "    \n",
    "- Understanding value ranges helps normalisation if needed and informs learning dynamics.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- The environment’s state consists of **4 continuous values**:\n",
    "    \n",
    "    - cart position, cart velocity, pole angle, and pole angular velocity.\n",
    "        \n",
    "- The environment allows **2 discrete actions**: move left (`0`) or right (`1`).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m50\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import TensorFlow and Keras layers for building the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Build the Q-network model\n",
    "# Input: 4 features from the environment\n",
    "# Output: 2 Q-values (one for each possible action)\n",
    "model = Sequential([\n",
    "    # First hidden layer with 24 neurons and ReLU activation\n",
    "    Dense(24, input_shape=(4,), activation='relu'),\n",
    "    \n",
    "    # Second hidden layer with 24 neurons and ReLU activation\n",
    "    Dense(24, activation='relu'),\n",
    "    \n",
    "    # Output layer with 2 neurons (Q-values for each action)\n",
    "    Dense(2, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model with mean squared error loss (for predicting Q-values)\n",
    "# and Adam optimizer (adaptive, stable convergence)\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "# Display model summary to verify structure\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`import tensorflow as tf` and layers**:  \n",
    "    Imports TensorFlow and the necessary components from Keras for building the neural network.\n",
    "    \n",
    "- **`model = Sequential([...])`**:  \n",
    "    Creates a **sequential neural network**, meaning layers are added one after the other.\n",
    "    \n",
    "- **`Dense(24, input_shape=(4,), activation='relu')`**:  \n",
    "    The first fully connected (dense) layer with:\n",
    "    \n",
    "    - 24 neurons\n",
    "        \n",
    "    - `input_shape=(4,)` means it expects 4 input features (cart position, cart velocity, pole angle, pole angular velocity)\n",
    "        \n",
    "    - `relu` activation introduces non-linearity so the network can learn complex patterns.\n",
    "        \n",
    "- **Second Dense layer**:\n",
    "    \n",
    "    - Another hidden layer with 24 neurons and `relu` activation, allowing the network to learn more complex relationships between inputs and Q-values.\n",
    "        \n",
    "- **Output Dense layer**:\n",
    "    \n",
    "    - 2 neurons (each representing the Q-value for one of the two actions: move left or move right)\n",
    "        \n",
    "    - Uses `linear` activation (default), as Q-values are continuous numbers without bounds.\n",
    "        \n",
    "- **`model.compile(loss='mse', optimizer=Adam(...))`**:\n",
    "    \n",
    "    - **Loss**: mean squared error (MSE) is used because we are predicting continuous values (Q-values) and minimising the difference between predicted and target values.\n",
    "        \n",
    "    - **Adam optimizer**: an adaptive gradient-based optimisation method that generally converges faster and more stably than alternatives.\n",
    "        \n",
    "    - Learning rate is set to **0.001**, a standard starting point.\n",
    "        \n",
    "- **`model.summary()`**:  \n",
    "    Displays the structure and parameter counts for each layer.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- I need a function approximator (Q-network) that takes the environment state and predicts Q-values for both actions.\n",
    "    \n",
    "- Having two hidden layers of reasonable size (24 neurons) gives enough complexity to capture the patterns without making the model too large or prone to overfitting.\n",
    "    \n",
    "- Using **MSE loss** and **Adam** helps the model learn stable and accurate predictions of Q-values over many episodes.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- **120 parameters in the first layer**:\n",
    "    \n",
    "    - `(4 input features + 1 bias) * 24 neurons = 120`\n",
    "        \n",
    "- **600 parameters in the second layer**:\n",
    "    \n",
    "    - `(24 input neurons + 1 bias) * 24 neurons = 600`\n",
    "        \n",
    "- **50 parameters in the output layer**:\n",
    "    \n",
    "    - `(24 input neurons + 1 bias) * 2 output neurons = 50`\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay memory created with capacity: 2000\n"
     ]
    }
   ],
   "source": [
    "# Import deque for creating a memory buffer\n",
    "from collections import deque\n",
    "\n",
    "# Set up replay memory to store past experiences (state, action, reward, next_state, done)\n",
    "# This allows the agent to learn from past experiences and break correlation between steps\n",
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "print(\"Replay memory created with capacity:\", replay_memory.maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`from collections import deque`**  \n",
    "    Imports the `deque` data structure from Python’s `collections` module.\n",
    "    \n",
    "    - A `deque` (double-ended queue) allows for efficient addition and removal of elements from both ends.\n",
    "        \n",
    "- **`replay_memory = deque(maxlen=2000)`**  \n",
    "    Creates a **replay memory buffer** with a maximum length of **2000 experiences**.\n",
    "    \n",
    "    - Each experience will later be stored as a tuple: `(state, action, reward, next_state, done)`\n",
    "        \n",
    "    - Once the buffer reaches 2000 entries, the oldest experiences are automatically removed to make room for new ones.\n",
    "        \n",
    "- **`print(\"Replay memory created with capacity:\", replay_memory.maxlen)`**  \n",
    "    Prints out the capacity of the replay memory to confirm that it has been created correctly.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- The **replay memory** is crucial for Deep Q-Learning.\n",
    "    \n",
    "- Instead of learning from consecutive steps, the model will randomly sample experiences from this buffer.\n",
    "    \n",
    "- This random sampling breaks correlations, stabilises learning, and allows the neural network to generalise better.\n",
    "    \n",
    "- Using a fixed-size `deque` makes memory management automatic and efficient.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- Confirms that the replay memory is set up and ready to store up to 2000 past experiences.\n",
    "    \n",
    "- This buffer will be filled during training as the agent interacts with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters set:\n",
      "gamma=0.95, epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.8, batch_size=32, episodes=75\n"
     ]
    }
   ],
   "source": [
    "# Set key hyperparameters for training\n",
    "\n",
    "# Discount factor (gamma) — determines how much future rewards are valued\n",
    "gamma = 0.95  \n",
    "\n",
    "# Initial exploration rate (epsilon) — probability of choosing a random action\n",
    "epsilon = 1.0  \n",
    "\n",
    "# Minimum exploration rate — ensures some exploration continues throughout training\n",
    "epsilon_min = 0.01  \n",
    "\n",
    "# Decay rate for epsilon — reduces exploration over time as the agent learns\n",
    "epsilon_decay = 0.800  \n",
    "\n",
    "# Batch size for training from replay memory\n",
    "batch_size = 32  \n",
    "\n",
    "# Number of episodes (full runs of the environment) to train over\n",
    "episodes = 75  \n",
    "\n",
    "print(\"Hyperparameters set:\")\n",
    "print(f\"gamma={gamma}, epsilon_start={epsilon}, epsilon_min={epsilon_min}, epsilon_decay={epsilon_decay}, batch_size={batch_size}, episodes={episodes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`gamma = 0.95`**\n",
    "    \n",
    "    - The **discount factor**.\n",
    "        \n",
    "    - It determines how much future rewards are valued compared to immediate rewards.\n",
    "        \n",
    "    - A gamma of **0.95** means the agent values future rewards highly but slightly less than immediate rewards.\n",
    "        \n",
    "    - A higher gamma (close to 1) makes the agent plan long-term, while a lower gamma makes it short-sighted.\n",
    "        \n",
    "- **`epsilon = 1.0`**\n",
    "    \n",
    "    - The **initial exploration rate**.\n",
    "        \n",
    "    - This means the agent will start by taking actions completely at random (100%) to explore the environment.\n",
    "        \n",
    "    - High exploration at the start is important to avoid biasing toward early random experiences.\n",
    "        \n",
    "- **`epsilon_min = 0.01`**\n",
    "    \n",
    "    - The **minimum exploration rate**.\n",
    "        \n",
    "    - Even after training for a while, the agent will still take random actions 1% of the time to avoid becoming too rigid and getting stuck in local optima.\n",
    "        \n",
    "- **`epsilon_decay = 0.995`**\n",
    "    \n",
    "    - The rate at which epsilon gradually decreases after each episode.\n",
    "        \n",
    "    - A decay factor of **0.995** means that after each episode, epsilon is multiplied by 0.995, slowly moving from 1.0 down to 0.01.\n",
    "        \n",
    "    - This allows the agent to shift from exploration (random moves) to exploitation (taking the best-known actions).\n",
    "        \n",
    "- **`batch_size = 64`**\n",
    "    \n",
    "    - When training from the replay memory, the agent will randomly sample 64 experiences at a time to update the model.\n",
    "        \n",
    "    - This balances training stability and speed; smaller batches train faster but can be less stable, larger batches slow training down.\n",
    "        \n",
    "- **`episodes = 500`**\n",
    "    \n",
    "    - The number of complete runs (episodes) of the environment the agent will train for.\n",
    "        \n",
    "    - Each episode runs until the pole falls or the time limit is reached.\n",
    "        \n",
    "    - 500 is a good number for CartPole — long enough for the agent to converge to good behaviour.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- Hyperparameters control how the agent learns.\n",
    "    \n",
    "- They balance exploration vs. exploitation, determine long-term vs. short-term rewards, and affect learning speed and stability.\n",
    "    \n",
    "- Without carefully chosen hyperparameters, the agent can:\n",
    "    \n",
    "    - Fail to learn (too little exploration or short-sighted gamma).\n",
    "        \n",
    "    - Overexplore and not exploit (high epsilon without decay).\n",
    "        \n",
    "    - Become stuck or slow (poor batch sizes).\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- Confirms that all learning parameters are correctly set.\n",
    "    \n",
    "- The agent will start exploring randomly, slowly learn to exploit actions that lead to high rewards, and value future rewards while training in batches of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to choose an action based on the current state\n",
    "def choose_action(state, epsilon):\n",
    "    # With probability epsilon, take a random action\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # Predict the Q-values for the state and choose the action with the highest Q-value (exploitation)\n",
    "        q_values = model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])  # Return the action with the highest predicted Q-value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **`def choose_action(state, epsilon):`**  \n",
    "    Defines a function that takes in the current environment state and the current exploration rate `epsilon`.\n",
    "    \n",
    "- **`if np.random.rand() <= epsilon:`**\n",
    "    \n",
    "    - Generates a random number between 0 and 1.\n",
    "        \n",
    "    - If it’s less than or equal to `epsilon`, the agent takes a **random action** — this is exploration.\n",
    "        \n",
    "    - High `epsilon` means more frequent random moves at the beginning.\n",
    "        \n",
    "- **`env.action_space.sample()`**\n",
    "    \n",
    "    - This randomly picks an action from the environment's action space.\n",
    "        \n",
    "    - In CartPole, this is either **0 (move left)** or **1 (move right)**.\n",
    "        \n",
    "- **`else:`**\n",
    "    \n",
    "    - If the random number is greater than `epsilon`, the agent uses the model to predict the best action — exploitation.\n",
    "        \n",
    "- **`q_values = model.predict(state, verbose=0)`**\n",
    "    \n",
    "    - The Q-network predicts the expected reward (Q-values) for both actions given the current state.\n",
    "        \n",
    "    - Output will be an array with two values: one for action 0, one for action 1.\n",
    "        \n",
    "- **`return np.argmax(q_values[0])`**\n",
    "    \n",
    "    - The action corresponding to the highest Q-value is chosen, meaning the model selects the action it believes will lead to the best future reward.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- The agent needs to balance **exploration** (trying new actions) and **exploitation** (choosing the best-known action).\n",
    "    \n",
    "- This function applies the **epsilon-greedy strategy**, the standard method for balancing these two.\n",
    "    \n",
    "- By wrapping it in a function, the main loop stays clean, and it’s easy to tweak or replace the action-selection logic if needed.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model using random samples from the replay memory\n",
    "def replay(batch_size):\n",
    "    # Don't train until there is enough experiences to fill a batch\n",
    "    if len(replay_memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Randomly sample a batch of experiences from the memory\n",
    "    minibatch = np.random.choice(len(replay_memory), batch_size, replace=False)\n",
    "    \n",
    "    # Prepare lists for states and target Q-values\n",
    "    states = []\n",
    "    targets = []\n",
    "    \n",
    "    # Process each sampled experience\n",
    "    for index in minibatch:\n",
    "        state, action, reward, next_state, done = replay_memory[index]\n",
    "        \n",
    "        # Predict current Q-values for the state\n",
    "        target = model.predict(state, verbose=0)[0]\n",
    "        \n",
    "        # If the episode is done, set Q-value for that action to reward only\n",
    "        if done:\n",
    "            target[action] = reward\n",
    "        else:\n",
    "            # Predict Q-values for the next state and use max Q-value for target update\n",
    "            t = model.predict(next_state, verbose=0)[0]\n",
    "            target[action] = reward + gamma * np.amax(t)\n",
    "        \n",
    "        states.append(state[0])\n",
    "        targets.append(target)\n",
    "    \n",
    "    # Train the model on the entire batch in one go\n",
    "    model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- `def replay(batch_size):`  \n",
    "    This function takes a `batch_size` and trains the model by using random experiences from the replay memory.\n",
    "    \n",
    "- `if len(replay_memory) < batch_size: return`  \n",
    "    It first checks if there are enough experiences in memory. If not, it returns.\n",
    "    \n",
    "- `minibatch = np.random.choice(len(replay_memory), batch_size, replace=False)`  \n",
    "    Randomly selects a set of indices from the replay memory without repetition, creating a mini-batch of experiences to learn from.\n",
    "    \n",
    "- `states = []` and `targets = []`  \n",
    "    These empty lists will store input states and the corresponding target Q-values for training.\n",
    "    \n",
    "- `for index in minibatch:`  \n",
    "    Iterates over each selected experience.\n",
    "    \n",
    "    - `state, action, reward, next_state, done = replay_memory[index]` extracts each experience tuple.\n",
    "        \n",
    "    - `target = model.predict(state, verbose=0)[0]` gets the current Q-values for the given state.\n",
    "        \n",
    "    - If the episode has ended (`done` is `True`), the Q-value for that action is set to the immediate reward.\n",
    "        \n",
    "    - Otherwise, it adds the reward plus the discounted future reward, calculated as `reward + gamma * max predicted Q-value of the next state`.\n",
    "        \n",
    "    - The state and calculated target Q-values are stored for batch training.\n",
    "        \n",
    "- `model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)`  \n",
    "    After processing the entire batch, the model is trained on these states and updated Q-values all at once for one epoch.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- This function allows the neural network to learn from many past experiences, not just the most recent action.\n",
    "    \n",
    "- It helps **break the correlation** between consecutive experiences by training on random samples.\n",
    "    \n",
    "- This stabilises learning and prevents the model from overfitting to patterns in sequential data.\n",
    "    \n",
    "- The logic for updating Q-values follows the **Bellman Equation**, using the reward and the estimated best future action.\n",
    "    \n",
    "- By training on batches, we allow more stable, generalised learning rather than on single data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/75, Total Reward: 21.0, Epsilon: 0.800\n",
      "Episode: 2/75, Total Reward: 18.0, Epsilon: 0.640\n",
      "Episode: 3/75, Total Reward: 68.0, Epsilon: 0.512\n",
      "Episode: 4/75, Total Reward: 42.0, Epsilon: 0.410\n",
      "Episode: 5/75, Total Reward: 41.0, Epsilon: 0.328\n",
      "Episode: 6/75, Total Reward: 63.0, Epsilon: 0.262\n",
      "Episode: 7/75, Total Reward: 76.0, Epsilon: 0.210\n",
      "Episode: 8/75, Total Reward: 47.0, Epsilon: 0.168\n",
      "Episode: 9/75, Total Reward: 234.0, Epsilon: 0.134\n",
      "Episode: 10/75, Total Reward: 62.0, Epsilon: 0.107\n",
      "Episode: 11/75, Total Reward: 72.0, Epsilon: 0.086\n",
      "Episode: 12/75, Total Reward: 57.0, Epsilon: 0.069\n",
      "Episode: 13/75, Total Reward: 37.0, Epsilon: 0.055\n",
      "Episode: 14/75, Total Reward: 51.0, Epsilon: 0.044\n",
      "Episode: 15/75, Total Reward: 69.0, Epsilon: 0.035\n",
      "Episode: 16/75, Total Reward: 32.0, Epsilon: 0.028\n",
      "Episode: 17/75, Total Reward: 67.0, Epsilon: 0.023\n",
      "Episode: 18/75, Total Reward: 28.0, Epsilon: 0.018\n",
      "Episode: 19/75, Total Reward: 18.0, Epsilon: 0.014\n",
      "Episode: 20/75, Total Reward: 31.0, Epsilon: 0.012\n",
      "Episode: 21/75, Total Reward: 30.0, Epsilon: 0.009\n",
      "Episode: 22/75, Total Reward: 11.0, Epsilon: 0.009\n",
      "Episode: 23/75, Total Reward: 14.0, Epsilon: 0.009\n",
      "Episode: 24/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 25/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 26/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 27/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 28/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 29/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 30/75, Total Reward: 12.0, Epsilon: 0.009\n",
      "Episode: 31/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 32/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 33/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 34/75, Total Reward: 25.0, Epsilon: 0.009\n",
      "Episode: 35/75, Total Reward: 15.0, Epsilon: 0.009\n",
      "Episode: 36/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 37/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 38/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 39/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 40/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 41/75, Total Reward: 8.0, Epsilon: 0.009\n",
      "Episode: 42/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 43/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 44/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 45/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 46/75, Total Reward: 9.0, Epsilon: 0.009\n",
      "Episode: 47/75, Total Reward: 10.0, Epsilon: 0.009\n",
      "Episode: 48/75, Total Reward: 8.0, Epsilon: 0.009\n",
      "Episode: 49/75, Total Reward: 11.0, Epsilon: 0.009\n",
      "Episode: 50/75, Total Reward: 40.0, Epsilon: 0.009\n",
      "Episode: 51/75, Total Reward: 54.0, Epsilon: 0.009\n",
      "Episode: 52/75, Total Reward: 53.0, Epsilon: 0.009\n",
      "Episode: 53/75, Total Reward: 50.0, Epsilon: 0.009\n",
      "Episode: 54/75, Total Reward: 33.0, Epsilon: 0.009\n",
      "Episode: 55/75, Total Reward: 61.0, Epsilon: 0.009\n",
      "Episode: 56/75, Total Reward: 53.0, Epsilon: 0.009\n",
      "Episode: 57/75, Total Reward: 25.0, Epsilon: 0.009\n",
      "Episode: 58/75, Total Reward: 25.0, Epsilon: 0.009\n",
      "Episode: 59/75, Total Reward: 27.0, Epsilon: 0.009\n",
      "Episode: 60/75, Total Reward: 26.0, Epsilon: 0.009\n",
      "Episode: 61/75, Total Reward: 32.0, Epsilon: 0.009\n",
      "Episode: 62/75, Total Reward: 53.0, Epsilon: 0.009\n",
      "Episode: 63/75, Total Reward: 77.0, Epsilon: 0.009\n",
      "Episode: 64/75, Total Reward: 62.0, Epsilon: 0.009\n",
      "Episode: 65/75, Total Reward: 25.0, Epsilon: 0.009\n",
      "Episode: 66/75, Total Reward: 33.0, Epsilon: 0.009\n",
      "Episode: 67/75, Total Reward: 38.0, Epsilon: 0.009\n",
      "Episode: 68/75, Total Reward: 40.0, Epsilon: 0.009\n",
      "Episode: 69/75, Total Reward: 39.0, Epsilon: 0.009\n",
      "Episode: 70/75, Total Reward: 24.0, Epsilon: 0.009\n",
      "Episode: 71/75, Total Reward: 28.0, Epsilon: 0.009\n",
      "Episode: 72/75, Total Reward: 21.0, Epsilon: 0.009\n",
      "Episode: 73/75, Total Reward: 23.0, Epsilon: 0.009\n",
      "Episode: 74/75, Total Reward: 22.0, Epsilon: 0.009\n",
      "Episode: 75/75, Total Reward: 18.0, Epsilon: 0.009\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]  # Reset environment at start of episode\n",
    "    state = np.reshape(state, [1, 4])  # Reshape for model input\n",
    "    total_reward = 0  # Track total reward for the episode\n",
    "    \n",
    "    for step in range(500):  # Limit steps to avoid infinite loops\n",
    "        # Choose action based on current state and epsilon\n",
    "        action = choose_action(state, epsilon)\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        \n",
    "        # Store experience in replay memory\n",
    "        replay_memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Train the network from memory\n",
    "        if step % 5 == 0:\n",
    "            replay(batch_size)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    # Print progress every 50 episodes\n",
    "    if (episode + 1) % 1 == 0:\n",
    "        print(f\"Episode: {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- `for episode in range(episodes):`  \n",
    "    Starts the main training loop, running for a set number of episodes. Each episode represents one full run of the environment until the pole falls or the time limit is reached.\n",
    "    \n",
    "- `state = env.reset()[0]`  \n",
    "    Resets the environment to its starting conditions and returns the initial state.  \n",
    "    The `[0]` index is used because `env.reset()` returns a tuple (state, info).\n",
    "    \n",
    "- `state = np.reshape(state, [1, 4])`  \n",
    "    Reshapes the state array to the correct shape for feeding into the neural network (1 row, 4 features).\n",
    "    \n",
    "- `total_reward = 0`  \n",
    "    Initialises a counter to track the total reward earned during the episode.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "- `for step in range(500):`  \n",
    "    Limits each episode to 500 steps to avoid infinite runs if the agent manages to balance the pole for a long time.\n",
    "    \n",
    "- `action = choose_action(state, epsilon)`  \n",
    "    Uses the previously defined epsilon-greedy function to choose the next action.\n",
    "    \n",
    "    - Either a random action (exploration) or the model’s predicted best action (exploitation).\n",
    "        \n",
    "- `next_state, reward, done, _, _ = env.step(action)`  \n",
    "    Executes the chosen action in the environment.\n",
    "    \n",
    "    - `next_state`: the new environment state.\n",
    "        \n",
    "    - `reward`: the reward given for the action.\n",
    "        \n",
    "    - `done`: `True` if the episode has ended.\n",
    "        \n",
    "    - `_`: placeholders for values returned but not used.\n",
    "        \n",
    "- `next_state = np.reshape(next_state, [1, 4])`  \n",
    "    Reshapes the next state to be compatible with the model input format.\n",
    "    \n",
    "- `replay_memory.append((state, action, reward, next_state, done))`  \n",
    "    Stores the experience in the replay buffer for future training.\n",
    "    \n",
    "- `state = next_state`  \n",
    "    Updates the current state for the next step.\n",
    "    \n",
    "- `total_reward += reward`  \n",
    "    Adds the reward from this step to the running total.\n",
    "    \n",
    "- `replay(batch_size)`  \n",
    "    Calls the replay function to train the model on a random batch of past experiences.\n",
    "    \n",
    "- `if done: break`  \n",
    "    Ends the episode early if the environment signals that the pole has fallen or time limit hit.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "- `if epsilon > epsilon_min: epsilon *= epsilon_decay`  \n",
    "    Reduces the value of epsilon after each episode, gradually shifting from exploration to exploitation.\n",
    "    \n",
    "- `if (episode + 1) % 50 == 0:`  \n",
    "    Every 50 episodes, prints out the progress:\n",
    "    \n",
    "    - Episode number\n",
    "        \n",
    "    - Total reward earned in that episode\n",
    "        \n",
    "    - Current value of epsilon (exploration rate).\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### Why this step\n",
    "\n",
    "- This loop is the core of reinforcement learning, where the agent:\n",
    "    \n",
    "    - Observes the state\n",
    "        \n",
    "    - Takes actions\n",
    "        \n",
    "    - Collects rewards\n",
    "        \n",
    "    - Stores experience\n",
    "        \n",
    "    - Learns from past experiences via replay\n",
    "        \n",
    "    - Gradually becomes more confident (reducing exploration with epsilon decay)\n",
    "        \n",
    "- Without this loop, the neural network would not learn how to maximise the reward or improve its Q-value predictions.\n",
    "    \n",
    "- Limiting episode length and decaying epsilon ensures efficient, stable training and avoids endless exploration.\n",
    "\n",
    "### Result \n",
    "\n",
    "- The training began with high exploration (epsilon starting at **0.8**) and early episodes show relatively low but improving total rewards, such as **21.0**, **18.0**, and peaks like **234.0** in episode 9.\n",
    "    \n",
    "- As **epsilon decayed**, the agent became more exploitative (relying on learned Q-values rather than random actions). Between episodes **1 to 20**, we observe improvement and occasional high scores, indicating that learning was taking place.\n",
    "    \n",
    "- After **episode 20**, where epsilon approached its minimum (around **0.009**), the model’s behaviour stabilised.\n",
    "    \n",
    "- However, rewards dropped to around **9–12** for many episodes between **20 and 50**. This suggests:\n",
    "    \n",
    "    - The agent might have **overfit** to certain strategies or struggled to find consistent policies due to limited training episodes and early aggressive decay.\n",
    "        \n",
    "    - There could be insufficient replay memory size or too few training episodes to converge properly.\n",
    "        \n",
    "- In the last quarter of the training, the rewards improved again (for example, **40**, **54**, **77**, and **62**). This shows the agent partially recovered by refining its learned Q-values toward the end of training.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Strengths**:\n",
    "    \n",
    "    - The agent demonstrated the ability to learn and achieve high scores at times.\n",
    "        \n",
    "    - Short spikes in performance (scores above **50–200**) indicate the neural network and replay memory worked correctly.\n",
    "        \n",
    "- **Weaknesses**:\n",
    "    \n",
    "    - Inconsistent performance throughout the middle section of training shows that the model likely needed **more episodes, slower epsilon decay, or a larger replay memory** to generalise better.\n",
    "        \n",
    "    - Stalling around low rewards during mid-training could be caused by excessive exploitation before the model was fully trained.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward during evaluation: 18.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent by running a test episode\n",
    "state = env.reset()[0]\n",
    "state = np.reshape(state, [1, 4])\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(500):\n",
    "    env.render()  # Visualise the agent's performance\n",
    "    action = np.argmax(model.predict(state, verbose=0)[0])  # Choose best action (pure exploitation)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    state = np.reshape(next_state, [1, 4])\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward during evaluation: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "- **Reset the environment** to start a new test run.\n",
    "    \n",
    "- **Reshape the state** for compatibility with the model’s input layer.\n",
    "    \n",
    "- Run a loop for a maximum of **500 steps**, which is the environment’s default cap for an episode.\n",
    "    \n",
    "- At each step:\n",
    "    \n",
    "    - **Render** the environment to visualise what the agent is doing.\n",
    "        \n",
    "    - Predict Q-values for the current state and select the action with the highest Q-value (no exploration now, just exploitation).\n",
    "        \n",
    "    - **Take the action** in the environment, receive the new state and reward, and track if it’s done.\n",
    "        \n",
    "    - Accumulate the total reward until the pole falls or the time limit is reached.\n",
    "        \n",
    "- Finally, close the environment and print out the total reward achieved.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Why?\n",
    "\n",
    "- This test phase checks the model’s real performance without randomness, using the learned policy.\n",
    "    \n",
    "- It shows how well the agent can keep the pole balanced based purely on its training (without epsilon-driven exploration).\n",
    "    \n",
    "- The total reward measures how long the agent can balance the pole, with a perfect score being close to 500.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "- The final evaluation run resulted in a **total reward of 18.0**.\n",
    "    \n",
    "- This means the trained agent was only able to keep the pole balanced for **18 steps** before failing.\n",
    "\n",
    "--- \n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- A reward of 18 is very low compared to the environment’s cap of 500 steps.\n",
    "    \n",
    "- This shows the agent **did not successfully learn a stable policy** for balancing the pole.\n",
    "    \n",
    "- The agent’s performance remained close to its early training levels and never converged to a strong policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "- **Environment:** OpenAI Gym’s **CartPole-v1** environment.\n",
    "    \n",
    "- **Description:** The task involves balancing a pole on a moving cart by applying forces left or right. The agent receives observations (cart position, cart velocity, pole angle, and pole angular velocity) and must choose actions to maximise how long it balances the pole.\n",
    "    \n",
    "- **Observation Space:**\n",
    "    \n",
    "    - 4 continuous variables:\n",
    "        \n",
    "        - Cart position\n",
    "            \n",
    "        - Cart velocity\n",
    "            \n",
    "        - Pole angle\n",
    "            \n",
    "        - Pole angular velocity\n",
    "            \n",
    "- **Action Space:**\n",
    "    \n",
    "    - Discrete(2):\n",
    "        \n",
    "        - Action 0: Move cart left\n",
    "            \n",
    "        - Action 1: Move cart right\n",
    "            \n",
    "\n",
    "**References**\n",
    "\n",
    "- https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "    \n",
    "- https://www.gymlibrary.dev/\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## Pre-Processing\n",
    "\n",
    "- No dataset preprocessing is required because the data is generated dynamically through the environment.\n",
    "    \n",
    "- **State reshaping:** States were reshaped from `(4,)` to `(1,4)` to match the input format expected by the neural network.\n",
    "    \n",
    "- **Replay Memory:** Created using a `deque` to store state transitions and sample them randomly for training, preventing correlated updates.\n",
    "    \n",
    "**Hyperparameter Definition**\n",
    "\n",
    "- Key parameters such as:\n",
    "    \n",
    "    - `gamma` (discount factor)\n",
    "        \n",
    "    - `epsilon` (exploration rate)\n",
    "        \n",
    "    - `epsilon_decay`\n",
    "        \n",
    "    - `epsilon_min`\n",
    "        \n",
    "    - `batch_size`\n",
    "        \n",
    "    - `episodes`\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## Data Visualisation and Understanding\n",
    "\n",
    "- **Observation Space Visualisation:** Displayed the boundaries and structure of the observation space to understand what ranges the agent works within.\n",
    "    \n",
    "- **Action space** of 2 discrete actions (left or right).\n",
    "    \n",
    "- **State Example:** Observed and reshaped states to confirm they align with input dimensions.\n",
    "    \n",
    "- **Episode Reward Monitoring:** Episode-by-episode rewards were printed to monitor performance improvements.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "- **Deep Q-Learning (DQN)**\n",
    "    \n",
    "    - Chosen due to its ability to handle continuous state spaces.\n",
    "        \n",
    "    - Utilises a Q-function approximator (neural network) instead of tabular Q-learning.\n",
    "        \n",
    "    - The Q-network predicts Q-values for each possible action given the current state.\n",
    "        \n",
    "    - Uses the Bellman Equation to update future reward estimates.\n",
    "        \n",
    "\n",
    "**- **Neural Network Structure**\n",
    "    \n",
    "    - Built with Keras `Sequential` model:\n",
    "        \n",
    "        - **Dense(24, ReLU)** layer for input and initial feature extraction.\n",
    "            \n",
    "        - Another **Dense(24, ReLU)** hidden layer for deeper feature learning.\n",
    "            \n",
    "        - Final **Dense(2, linear)** layer outputting Q-values for both actions.\n",
    "            \n",
    "    - **Adam optimiser** used for adaptive learning.\n",
    "        \n",
    "- **Exploration vs Exploitation**\n",
    "    \n",
    "    - Epsilon-greedy strategy applied, with `epsilon` decaying over time.\n",
    "\n",
    "**Bellman Equation Reference:**\n",
    "\n",
    "- https://www.geeksforgeeks.org/bellman-equation/\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## **Model Training and Evaluation**\n",
    "\n",
    "- **Training Loop**:\n",
    "    \n",
    "    - Ran for up to **75 episodes** in the final configuration.\n",
    "        \n",
    "    - Each step:\n",
    "        \n",
    "        - Took an action based on epsilon-greedy policy.\n",
    "            \n",
    "        - Observed reward and next state.\n",
    "            \n",
    "        - Stored experience in replay memory.\n",
    "            \n",
    "        - Trained model on random batches from memory.\n",
    "            \n",
    "    - **Epsilon decay**: aggressively reduced to speed up training, but led to early convergence on suboptimal strategies.\n",
    "        \n",
    "- **Final Evaluation**:\n",
    "    \n",
    "    - The trained agent achieved a total reward of **18** during evaluation — far from the 195+ required for solved status.\n",
    "\n",
    "--- \n",
    "\n",
    "## Online Resources and Sources\n",
    "\n",
    "- https://www.geeksforgeeks.org/q-learning-in-python/\n",
    "    \n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    \n",
    "- https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "    \n",
    "- https://www.gymlibrary.dev/\n",
    "    \n",
    "- https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "    \n",
    "- https://medium.com/@alwinraju/in-depth-guide-to-implementing-q-learning-in-python-with-openai-gym-s-taxi-environment-cd356cc6a288\n",
    "    \n",
    "- https://aleksandarhaber.com/deep-q-networks-dqn-in-python-from-scratch-by-using-openai-gym-and-tensorflow-reinforcement-learning-tutorial/\n",
    "    \n",
    "- https://www.geeksforgeeks.org/bellman-equation/\n",
    "    \n",
    "- **ChatGPT (chat.openai.com)** – for help understanding code structures, explanations of functions, and expanding on what was provided by tutorials.\n",
    "  \n",
    "---\n",
    "## **Tools and Technologies**\n",
    "\n",
    "- **Python 3** — Used for all coding tasks.\n",
    "    \n",
    "- **NumPy** — For numeric computations and random sampling.\n",
    "    \n",
    "- **OpenAI Gym** — Environment simulation.\n",
    "    \n",
    "- **TensorFlow & Keras** — For building the neural network and prediction functions.\n",
    "    \n",
    "- **Deque** — To manage experience replay memory efficiently.\n",
    "    \n",
    "- **Jupyter Notebook** — Used for incremental development and code execution.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## **Challenges Faced**\n",
    "\n",
    "1. **Understanding Reinforcement Learning Concepts**\n",
    "    \n",
    "    - Initially struggled with the theory of **Q-learning** and how **Q-function approximation** works with neural networks.\n",
    "        \n",
    "    - Required extensive reading of online tutorials and breaking down each part of the Bellman equation.\n",
    "        \n",
    "2. **Code Complexity**\n",
    "    \n",
    "    - Functions like `choose_action()` and `replay()` were initially confusing, especially the way targets and rewards are calculated.\n",
    "        \n",
    "    - Needed help from ChatGPT and multiple online sources to expand and fully understand each line.\n",
    "        \n",
    "3. **Runtime Constraints**\n",
    "    \n",
    "    - Training was **extremely slow** on CPU-only hardware.\n",
    "        \n",
    "    - Original setups with **500 episodes** and larger batch sizes were impossible to complete within a reasonable time frame.\n",
    "        \n",
    "    - Reduced episodes, batch size, and replay memory to speed things up, sacrificing training quality.\n",
    "        \n",
    "4. **Accuracy and Performance**\n",
    "    \n",
    "    - Despite tuning hyperparameters like `gamma`, `epsilon`, and decay rates, the final model could not reach a reward that indicates learning success.\n",
    "        \n",
    "    - This was due to both aggressive hyperparameter cuts and hardware limitations.\n",
    "        \n",
    "    - The final evaluation run returned a reward of **18**, demonstrating limited learning.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "- A **Deep Q-Network (DQN)** was successfully implemented and trained using OpenAI Gym’s **CartPole-v1** environment.\n",
    "    \n",
    "- Although the final agent did not achieve a high score, the process demonstrated a clear understanding of:\n",
    "    \n",
    "    - How states, actions, and rewards interact.\n",
    "        \n",
    "    - The role of exploration and exploitation.\n",
    "        \n",
    "    - The use of experience replay and neural networks for approximating Q-values.\n",
    "        \n",
    "- Significant constraints in CPU performance and runtime limited the model’s ability to reach full potential.\n",
    "    \n",
    "- With access to better hardware and more time for slower, stable training, performance could be substantially improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
