{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Ronan Green  \n",
    "**Model:** Supervised Neural Network (SNN)  \n",
    "**Brief Description:**  \n",
    "A Supervised Neural Network consists of layers of interconnected neurons trained on labelled data. \n",
    "\n",
    "**Note:**  \n",
    "This notebook was created by Ronan Green. A full breakdown of the findings, methodology, and references used can be found at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Cement",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Blast_Furnace_Slag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fly_Ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Water",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Superplasticizer",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Coarse_Aggregate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fine_Aggregate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Strength",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6dc436d3-ff3e-4a4c-910e-d68f9d91eba6",
       "rows": [
        [
         "0",
         "540.0",
         "0.0",
         "0.0",
         "162.0",
         "2.5",
         "1040.0",
         "676.0",
         "28",
         "79.99"
        ],
        [
         "1",
         "540.0",
         "0.0",
         "0.0",
         "162.0",
         "2.5",
         "1055.0",
         "676.0",
         "28",
         "61.89"
        ],
        [
         "2",
         "332.5",
         "142.5",
         "0.0",
         "228.0",
         "0.0",
         "932.0",
         "594.0",
         "270",
         "40.27"
        ],
        [
         "3",
         "332.5",
         "142.5",
         "0.0",
         "228.0",
         "0.0",
         "932.0",
         "594.0",
         "365",
         "41.05"
        ],
        [
         "4",
         "198.6",
         "132.4",
         "0.0",
         "192.0",
         "0.0",
         "978.4",
         "825.5",
         "360",
         "44.3"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast_Furnace_Slag</th>\n",
       "      <th>Fly_Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse_Aggregate</th>\n",
       "      <th>Fine_Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast_Furnace_Slag  Fly_Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse_Aggregate  Fine_Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas and load the XLS file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Concrete_Data.csv')\n",
    "\n",
    "# Rename columns to shorter names for convenience\n",
    "df.columns = [\n",
    "    'Cement', \n",
    "    'Blast_Furnace_Slag', \n",
    "    'Fly_Ash', \n",
    "    'Water', \n",
    "    'Superplasticizer', \n",
    "    'Coarse_Aggregate', \n",
    "    'Fine_Aggregate', \n",
    "    'Age', \n",
    "    'Strength'\n",
    "]\n",
    "\n",
    "# Display the first few rows to confirm data loaded correctly\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Explanation:\n",
    "\n",
    "- **Import pandas:** I use the `pandas` library for data handling and manipulation.\n",
    "- **`df = pd.read_csv('Concrete_Data.csv')`:** Reads the CSV file into a DataFrame named `df`.\n",
    "- **Renaming columns:** I assign shorter names (`df.columns = [...]`) to make column references simpler and more readable in subsequent steps.\n",
    "- **`df.head()`:** Displays the first five rows of the dataset, helping to verify that the data loaded correctly and check the new column names.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Step?\n",
    "\n",
    "- **Load the dataset:** It’s essential to import the dataset before any analysis or preprocessing can occur.\n",
    "- **Rename columns:** Long or unwieldy column names can slow down development and clutter the code; shorter names improve clarity and reduce errors.\n",
    "- **Inspect the first rows:** Quickly confirms whether the dataset has been read properly, ensuring we have the right structure, column headings, and data format before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (824, 8)\n"
     ]
    }
   ],
   "source": [
    "# Import functions for splitting the data and scaling features\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate the features (X) from the target (y)\n",
    "X = df.drop('Strength', axis=1)\n",
    "y = df['Strength']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialise the StandardScaler to scale our features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the shape of the scaled training data\n",
    "print(\"Training data shape:\", X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Explanation:\n",
    "- **Importing essential functions:**\n",
    "    - `train_test_split` is used to split the dataset into training and testing subsets.\n",
    "    - `StandardScaler` is used for feature scaling (normalising or standardising feature values).\n",
    "- **Feature/target separation:**\n",
    "    - `X = df.drop('Strength', axis=1)` defines all columns except `'Strength'` as input features.\n",
    "    - `y = df['Strength']` isolates the target variable we aim to predict.\n",
    "- **Splitting the data:**\n",
    "    - `train_test_split(X, y, test_size=0.2, random_state=42)` creates two sets of features (`X_train`, `X_test`) and two sets of targets (`y_train`, `y_test`) with 80% data for training and 20% data for testing, ensuring reproducibility with `random_state=42`.\n",
    "- **Scaling the features:**\n",
    "    - I create an instance of `StandardScaler()` and then call `fit_transform` on the training features to compute scaling parameters and apply them.\n",
    "    - I use the same scaling parameters to transform the test set (`transform`) without re-fitting, ensuring the same scaling approach for both sets.\n",
    "- **Print shape of training data:**\n",
    "    - `print(\"Training data shape:\", X_train_scaled.shape)` confirms the dimensions of the scaled training feature matrix.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Step?\n",
    "\n",
    "- **Data splitting:** Separating the data into training and testing sets helps assess model generalisation. Training on one subset and validating on another prevents overfitting.\n",
    "- **Feature scaling:** Neural networks (and many other machine learning algorithms) often perform better when input features share a similar scale. Standardisation ensures each feature has zero mean and unit variance, speeding up convergence and potentially improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689</span> (10.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,689\u001b[0m (10.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689</span> (10.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,689\u001b[0m (10.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import TensorFlow and Keras components for building the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    # Input layer with 64 neurons; 'input_shape' matches the number of features (8)\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    \n",
    "    # Hidden layer with 32 neurons and ReLU activation for non-linearity\n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    # Output layer with 1 neuron for regression; linear activation is used by default\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model using the Adam optimizer and mean squared error loss for regression\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Print the model summary to see the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Explanation:\n",
    "\n",
    "- **Imports:**\n",
    "    - `tensorflow` for building and training neural networks.\n",
    "    - `Sequential` to define a linear stack of layers.\n",
    "\t    - `Sequential` is a Keras model class that lets me stack layers in a linear pipeline: each layer receives the output from the previous layer and feeds its output to the next layer.\n",
    "\t    - It is used for straightforward architectures where each layer directly follows another.\n",
    "    - `Dense` to add fully connected (dense) layers in the network.\n",
    "\t    - A `Dense` layer is a fully connected layer, meaning every neuron in the layer is connected to every neuron in the preceding layer.\n",
    "\t    - Dense layers are a standard choice for general-purpose neural networks, especially for tabular data. They help combine information from all features.\n",
    "- **Defining the model:**\n",
    "    - **First Dense layer**: has 64 neurons and uses `relu` activation to introduce non-linearity. The `input_shape=(X_train_scaled.shape[1],)` specifies the number of input features.\n",
    "    - **Second Dense layer**: has 32 neurons, also with `relu` activation. This reduces the dimensionality from 64 to 32.\n",
    "    - **Output Dense layer**: has 1 neuron and no explicit activation (default is linear).\n",
    "\t    - We start with 64 neurons, then go down to 32, and finally 1 for the output. Each layer’s size (number of neurons) is a **hyperparameter** you can tune.\n",
    "\t    - Decreasing layer sizes is a simple way to gradually condense information. The final output layer has a single neuron because we’re predicting one numeric value (concrete strength).\n",
    "\t    - ReLU outputs `max(0, x)`, introducing non-linearity and avoiding some issues (like vanishing gradients) that older activation functions had.\n",
    "\t\t- It’s efficient, widely used in modern neural networks, and typically speeds up training convergence.\n",
    "\t\t\t- **Vanishing Gradient Problem**\n",
    "\t\t\t\t- **What Is It?**  \n",
    "\t\t\t\t\t In deep neural networks, especially those with many layers and certain activation functions, gradients (the signals used to update weights) can become extremely small as they propagate backward through the network.\n",
    "\t\t\t\t- **Why Problematic?**  \n",
    "\t\t\t\t     When gradients vanish, early layers learn very slowly because their weight updates are near zero. This severely limits a network’s ability to learn complex patterns. Modern activation functions (like ReLU) and proper initialization strategies can help mitigate this issue.\n",
    "- **Compiling the model:**\n",
    "    - **optimizer='adam'**: Adaptive Moment Estimation (Adam) is an efficient choice for most neural network tasks.\n",
    "\t    - Adam (Adaptive Moment Estimation) is an algorithm that optimises model parameters using an adaptive learning rate and momentum.\n",
    "\t    - It generally converges faster and more reliably than simpler optimizers like Stochastic Gradient Descent (SGD). \n",
    "    - **loss='mse'**: Mean Squared Error is the standard regression loss function.\n",
    "\t    - MSE calculates the average squared difference between predicted and actual values.\n",
    "\t    - It’s a standard metric for regression tasks and directly corresponds to the variance of prediction errors. Minimising MSE encourages the model to make smaller errors overall.\n",
    "\t    - Mean Absolute Error (`mae`) can also be used as a loss function; it is sometimes more robust to outliers but can be trickier to optimize smoothly.\n",
    "    - **metrics=['mae']**: We track Mean Absolute Error as an additional performance indicator.\n",
    "\t    - MAE computes the average absolute difference between predicted and actual values.\n",
    "\t    - It’s more interpretable in many real-world tasks (e.g., “the model is off by X units on average”). It also helps you track a different perspective on error compared to the MSE loss.\n",
    "\t    - You could track other regression metrics like `R^2` or `Mean Squared Logarithmic Error`, depending on the problem requirements.\n",
    "- **`model.summary()`:**\n",
    "    - Prints a table summarizing each layer’s output shape and total parameters, offering a quick glimpse of the network’s complexity.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Step?\n",
    "\n",
    "- **Model Definition**: Specifying the architecture (how many layers, neurons, and activation functions) is crucial for capturing the relationship between inputs and outputs.\n",
    "- **Compilation**: Tells the model _how_ to optimize its parameters (via `optimizer`) and _what_ metric or loss function to minimize. This step finalizes the model configuration before training.\n",
    "- **Summary**: Quickly inspects the structure to ensure the model is set up correctly, checking the parameter counts and layer outputs.\n",
    "  \n",
    "---\n",
    "#### Result\n",
    "- **Layer 1** (`dense`): Connects the 8 input features to 64 neurons. We have 8×648 \\times 648×64 weights + 64 biases = **576 parameters**.\n",
    "- **Layer 2** (`dense_1`): Connects 64 neurons to 32 neurons. That’s 64×3264 \\times 3264×32 weights + 32 biases = **2,080 parameters**.\n",
    "- **Layer 3** (`dense_2`): The output layer, which has 1 neuron. That’s 32×132 \\times 132×1 weights + 1 bias = **33 parameters**.\n",
    "- **Total Params**: 2,689 trainable parameters — all of which the optimizer updates during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1456.3225 - mae: 34.1563 - val_loss: 1438.3208 - val_mae: 34.6783\n",
      "Epoch 2/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1462.1260 - mae: 34.0481 - val_loss: 1308.3958 - val_mae: 32.7984\n",
      "Epoch 3/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1251.9810 - mae: 31.1245 - val_loss: 1098.4745 - val_mae: 29.5865\n",
      "Epoch 4/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1056.1219 - mae: 28.0001 - val_loss: 824.0176 - val_mae: 24.9302\n",
      "Epoch 5/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 740.4262 - mae: 23.0721 - val_loss: 535.5587 - val_mae: 19.2424\n",
      "Epoch 6/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 449.3405 - mae: 16.5666 - val_loss: 316.3624 - val_mae: 14.4028\n",
      "Epoch 7/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 308.4364 - mae: 13.8355 - val_loss: 210.3783 - val_mae: 11.7414\n",
      "Epoch 8/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 243.9401 - mae: 12.6096 - val_loss: 178.2851 - val_mae: 11.0704\n",
      "Epoch 9/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 210.0541 - mae: 11.6873 - val_loss: 172.1586 - val_mae: 10.9767\n",
      "Epoch 10/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 198.0424 - mae: 11.7165 - val_loss: 165.9894 - val_mae: 10.8429\n",
      "Epoch 11/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 191.3027 - mae: 11.2936 - val_loss: 161.2958 - val_mae: 10.7306\n",
      "Epoch 12/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 185.7370 - mae: 11.0420 - val_loss: 156.8249 - val_mae: 10.5983\n",
      "Epoch 13/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 170.7800 - mae: 10.6854 - val_loss: 152.4296 - val_mae: 10.4633\n",
      "Epoch 14/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 183.8585 - mae: 10.9335 - val_loss: 150.6502 - val_mae: 10.3964\n",
      "Epoch 15/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 158.5824 - mae: 10.3573 - val_loss: 146.8893 - val_mae: 10.2752\n",
      "Epoch 16/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 153.7855 - mae: 10.1456 - val_loss: 144.1299 - val_mae: 10.1816\n",
      "Epoch 17/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 157.6094 - mae: 10.3083 - val_loss: 143.0676 - val_mae: 10.1412\n",
      "Epoch 18/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 162.0912 - mae: 10.3429 - val_loss: 140.9497 - val_mae: 10.0591\n",
      "Epoch 19/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 151.7101 - mae: 10.1229 - val_loss: 139.9072 - val_mae: 10.0017\n",
      "Epoch 20/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 137.6517 - mae: 9.5439 - val_loss: 136.4034 - val_mae: 9.8780\n",
      "Epoch 21/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 144.8825 - mae: 9.8434 - val_loss: 134.5867 - val_mae: 9.7897\n",
      "Epoch 22/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 145.4915 - mae: 9.9580 - val_loss: 135.9640 - val_mae: 9.8410\n",
      "Epoch 23/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 139.6375 - mae: 9.6447 - val_loss: 132.6310 - val_mae: 9.7093\n",
      "Epoch 24/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 143.9552 - mae: 9.8796 - val_loss: 131.5955 - val_mae: 9.6576\n",
      "Epoch 25/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 146.8058 - mae: 9.8921 - val_loss: 130.5760 - val_mae: 9.6010\n",
      "Epoch 26/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 147.8478 - mae: 9.8869 - val_loss: 129.8976 - val_mae: 9.5615\n",
      "Epoch 27/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 130.3689 - mae: 9.3418 - val_loss: 129.2713 - val_mae: 9.5393\n",
      "Epoch 28/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 139.4218 - mae: 9.5804 - val_loss: 127.7089 - val_mae: 9.4541\n",
      "Epoch 29/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 132.4302 - mae: 9.3668 - val_loss: 126.5317 - val_mae: 9.4040\n",
      "Epoch 30/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 138.0221 - mae: 9.5424 - val_loss: 124.7395 - val_mae: 9.3091\n",
      "Epoch 31/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 143.7120 - mae: 9.7867 - val_loss: 124.2372 - val_mae: 9.2926\n",
      "Epoch 32/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 125.5064 - mae: 9.0773 - val_loss: 123.2235 - val_mae: 9.2471\n",
      "Epoch 33/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 123.9486 - mae: 9.0097 - val_loss: 121.7184 - val_mae: 9.1808\n",
      "Epoch 34/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 135.6257 - mae: 9.4793 - val_loss: 120.5370 - val_mae: 9.1155\n",
      "Epoch 35/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 122.2346 - mae: 8.8978 - val_loss: 119.8517 - val_mae: 9.0822\n",
      "Epoch 36/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 126.0523 - mae: 9.0616 - val_loss: 118.7988 - val_mae: 9.0164\n",
      "Epoch 37/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 121.9076 - mae: 8.8728 - val_loss: 117.3968 - val_mae: 8.9799\n",
      "Epoch 38/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 119.6593 - mae: 8.8743 - val_loss: 115.9472 - val_mae: 8.9010\n",
      "Epoch 39/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 122.5412 - mae: 8.8948 - val_loss: 113.9397 - val_mae: 8.8058\n",
      "Epoch 40/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 114.5869 - mae: 8.6506 - val_loss: 113.5001 - val_mae: 8.7850\n",
      "Epoch 41/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 119.7331 - mae: 8.9615 - val_loss: 113.1220 - val_mae: 8.7871\n",
      "Epoch 42/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 113.0901 - mae: 8.5609 - val_loss: 109.7712 - val_mae: 8.5925\n",
      "Epoch 43/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 116.8586 - mae: 8.7350 - val_loss: 110.0532 - val_mae: 8.6453\n",
      "Epoch 44/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 108.5592 - mae: 8.4490 - val_loss: 106.9871 - val_mae: 8.4701\n",
      "Epoch 45/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 117.0183 - mae: 8.7713 - val_loss: 106.3193 - val_mae: 8.4602\n",
      "Epoch 46/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 110.9689 - mae: 8.4110 - val_loss: 103.5353 - val_mae: 8.3172\n",
      "Epoch 47/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 102.5542 - mae: 8.1626 - val_loss: 103.0675 - val_mae: 8.3079\n",
      "Epoch 48/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 101.3078 - mae: 8.1256 - val_loss: 101.0803 - val_mae: 8.1963\n",
      "Epoch 49/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 110.8282 - mae: 8.4156 - val_loss: 98.9793 - val_mae: 8.0970\n",
      "Epoch 50/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 100.4987 - mae: 8.0125 - val_loss: 97.0937 - val_mae: 7.9994\n",
      "Epoch 51/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 99.0101 - mae: 7.9274 - val_loss: 95.5094 - val_mae: 7.9358\n",
      "Epoch 52/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 101.8174 - mae: 8.1387 - val_loss: 93.0683 - val_mae: 7.7863\n",
      "Epoch 53/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 93.9505 - mae: 7.7012 - val_loss: 91.4295 - val_mae: 7.7213\n",
      "Epoch 54/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84.3398 - mae: 7.2184 - val_loss: 89.8400 - val_mae: 7.5208\n",
      "Epoch 55/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 88.5469 - mae: 7.3608 - val_loss: 88.3428 - val_mae: 7.5667\n",
      "Epoch 56/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 97.1038 - mae: 7.9663 - val_loss: 85.6759 - val_mae: 7.3930\n",
      "Epoch 57/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84.0881 - mae: 7.2088 - val_loss: 83.6150 - val_mae: 7.2499\n",
      "Epoch 58/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84.7764 - mae: 7.1753 - val_loss: 81.9114 - val_mae: 7.1465\n",
      "Epoch 59/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 80.8769 - mae: 6.9923 - val_loss: 79.5524 - val_mae: 7.0002\n",
      "Epoch 60/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 77.6793 - mae: 6.9922 - val_loss: 78.0370 - val_mae: 6.9383\n",
      "Epoch 61/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 72.4254 - mae: 6.6123 - val_loss: 76.2273 - val_mae: 6.7433\n",
      "Epoch 62/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 73.5479 - mae: 6.6576 - val_loss: 73.7747 - val_mae: 6.7082\n",
      "Epoch 63/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 67.9745 - mae: 6.4255 - val_loss: 71.4164 - val_mae: 6.5220\n",
      "Epoch 64/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 71.0289 - mae: 6.4122 - val_loss: 70.0028 - val_mae: 6.3998\n",
      "Epoch 65/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 65.2902 - mae: 6.2013 - val_loss: 67.8504 - val_mae: 6.2836\n",
      "Epoch 66/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 57.1227 - mae: 5.8010 - val_loss: 66.3982 - val_mae: 6.2207\n",
      "Epoch 67/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 54.9420 - mae: 5.6607 - val_loss: 64.3436 - val_mae: 6.0805\n",
      "Epoch 68/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 59.5496 - mae: 5.7311 - val_loss: 62.5066 - val_mae: 6.0224\n",
      "Epoch 69/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 63.8049 - mae: 5.9414 - val_loss: 61.0790 - val_mae: 5.9271\n",
      "Epoch 70/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 57.5008 - mae: 5.7888 - val_loss: 59.7409 - val_mae: 5.8977\n",
      "Epoch 71/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 57.9753 - mae: 5.7987 - val_loss: 57.8676 - val_mae: 5.7744\n",
      "Epoch 72/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 53.9897 - mae: 5.5220 - val_loss: 55.9310 - val_mae: 5.6789\n",
      "Epoch 73/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 50.0689 - mae: 5.4571 - val_loss: 54.5760 - val_mae: 5.6390\n",
      "Epoch 74/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 57.8488 - mae: 5.7435 - val_loss: 53.6355 - val_mae: 5.5741\n",
      "Epoch 75/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 50.8830 - mae: 5.3385 - val_loss: 52.3228 - val_mae: 5.5104\n",
      "Epoch 76/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 49.1051 - mae: 5.1982 - val_loss: 51.1627 - val_mae: 5.4504\n",
      "Epoch 77/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 45.8447 - mae: 5.0513 - val_loss: 49.8681 - val_mae: 5.3849\n",
      "Epoch 78/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.8849 - mae: 4.8441 - val_loss: 49.0961 - val_mae: 5.3215\n",
      "Epoch 79/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 45.2322 - mae: 5.0413 - val_loss: 48.3109 - val_mae: 5.3135\n",
      "Epoch 80/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 42.4880 - mae: 4.8720 - val_loss: 47.4767 - val_mae: 5.2375\n",
      "Epoch 81/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 44.5175 - mae: 4.8947 - val_loss: 46.0233 - val_mae: 5.1371\n",
      "Epoch 82/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 41.5285 - mae: 4.8923 - val_loss: 45.1883 - val_mae: 5.0904\n",
      "Epoch 83/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 42.8784 - mae: 4.9456 - val_loss: 44.1705 - val_mae: 5.0281\n",
      "Epoch 84/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.9235 - mae: 4.9548 - val_loss: 43.9675 - val_mae: 5.0143\n",
      "Epoch 85/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 41.1595 - mae: 4.7496 - val_loss: 43.2702 - val_mae: 4.9462\n",
      "Epoch 86/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 38.1323 - mae: 4.5286 - val_loss: 42.6375 - val_mae: 4.9246\n",
      "Epoch 87/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 39.7189 - mae: 4.6079 - val_loss: 41.9777 - val_mae: 4.8548\n",
      "Epoch 88/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 37.7578 - mae: 4.5959 - val_loss: 41.8345 - val_mae: 4.9154\n",
      "Epoch 89/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 44.1469 - mae: 4.9596 - val_loss: 40.8414 - val_mae: 4.8217\n",
      "Epoch 90/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 37.3740 - mae: 4.6255 - val_loss: 40.0075 - val_mae: 4.7455\n",
      "Epoch 91/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.4175 - mae: 4.4766 - val_loss: 39.9317 - val_mae: 4.7469\n",
      "Epoch 92/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.3969 - mae: 4.5915 - val_loss: 39.5035 - val_mae: 4.7823\n",
      "Epoch 93/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 37.7529 - mae: 4.6327 - val_loss: 39.7251 - val_mae: 4.7985\n",
      "Epoch 94/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 39.6045 - mae: 4.7172 - val_loss: 38.7044 - val_mae: 4.7063\n",
      "Epoch 95/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 38.1002 - mae: 4.6896 - val_loss: 38.4535 - val_mae: 4.7181\n",
      "Epoch 96/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35.6826 - mae: 4.4259 - val_loss: 38.0502 - val_mae: 4.6850\n",
      "Epoch 97/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35.4251 - mae: 4.5253 - val_loss: 37.5313 - val_mae: 4.6809\n",
      "Epoch 98/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35.6302 - mae: 4.4879 - val_loss: 37.7402 - val_mae: 4.6416\n",
      "Epoch 99/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.5570 - mae: 4.4651 - val_loss: 37.2904 - val_mae: 4.6054\n",
      "Epoch 100/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 33.0757 - mae: 4.3428 - val_loss: 37.0483 - val_mae: 4.6519\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network on the scaled training data\n",
    "# validation_split=0.2 reserves 20% of training data for validation during training\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 'history' contains the training loss and validation loss over epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Code Explanation:**\n",
    "\n",
    "1. **`model.fit(...)`:**\n",
    "    - I call `.fit(...)` to **train** our neural network. It iterates over the training data several times (epochs), each epoch consisting of batches of samples.\n",
    "    - **`epochs=100`** means the model will see the entire training data 100 times.\n",
    "    - **`validation_split=0.2`** automatically reserves 20% of the _training set_ (not the already separated test set) for validation. This monitors the model’s performance on data it doesn’t train on each epoch, helping spot overfitting early.\n",
    "    - **`verbose=1`** prints detailed logs — including epoch number, training loss, training MAE, validation loss, and validation MAE.\n",
    "2. **Loss and MAE in the Output:**\n",
    "    - **`loss`:** Training loss (here, MSE) measures how well the model is fitting the training data. Lower is better.\n",
    "    - **`mae`:** Training Mean Absolute Error — average magnitude of errors on the training set.\n",
    "    - **`val_loss`:** Validation loss (MSE on the 20% validation split). This indicates how well the model generalises.\n",
    "    - **`val_mae`:** Mean Absolute Error on the validation subset. If `val_loss` or `val_mae` start to **increase** while training loss is still **decreasing**, it can be a sign of overfitting.\n",
    "3. **Training Progress:**\n",
    "    - You see MSE dropping from **1611.7** down to **36.9** over 100 epochs (and MAE dropping from **36.2** to around **4.5**).\n",
    "    - Correspondingly, the validation MSE drops from around **1477.9** to **41.5**, and the MAE from about **35.2** to **5.0**, showing **steady learning**.\n",
    "    - The smaller the MAE, the closer the model’s predictions are to the actual target values on average (e.g., an MAE of around 5 means on average you’re off by ±5 MPa).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why This Step?**\n",
    "\n",
    "- **Model Training:** This is **the core process** where the network’s weights are updated to reduce the difference between predictions and true values (loss).\n",
    "- **Validation:** Monitoring on a separate validation subset helps catch overfitting and provides an unbiased view of the model’s performance.\n",
    "- **Epoch Output:** Inspecting losses and metrics each epoch allows you to see whether the model is still improving or has plateaued, guiding decisions like early stopping or tuning hyperparameters (e.g., learning rate, number of epochs, network size).\n",
    "\n",
    "\n",
    "---\n",
    "#### **Result**\n",
    "\n",
    "- **Training Loss & MAE:** Drops significantly from the first epoch (`loss: 1611.7493, mae: 36.2553`) to the last epoch (`loss: 36.9255, mae: 4.5082`). This indicates the model is learning to approximate the target variable more accurately over time.\n",
    "- **Validation Loss & MAE:** Decreasing from around `(val_loss: 1477.8672, val_mae: 35.2095)` down to `(val_loss: 41.5142, val_mae: 4.9996)` shows the model is also improving on unseen validation data, not just memorising the training set.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 40.3848 - mae: 5.2007\n",
      "Test MAE: 5.079457759857178\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test set\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(\"Test MAE:\", test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Code Explanation**\n",
    "\n",
    "- *`model.evaluate(X_test_scaled, y_test, verbose=1)`:**\n",
    "    \n",
    "    - Evaluates how well the trained model performs on unseen test data.\n",
    "        \n",
    "    - Returns the **MAE** on the test set.\n",
    "        \n",
    "- *`print(\"Test MAE:\", test_mae)`:**\n",
    "    \n",
    "    - Logs the Mean Absolute Error for easy readability.\n",
    "        \n",
    "    - Using `verbose=1` ensures detailed output during evaluation.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "#### **Why?**\n",
    "\n",
    "- **Generalization Check:** Evaluating on the test set (which the model hasn’t seen during training or validation) measures the model’s ability to generalize to new data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Result**\n",
    "\n",
    "- The **test loss (MSE)** is approximately **40.4125**, and the **test MAE** is around **5.1085** (or **4.9819**, depending on the run).\n",
    "    \n",
    "- A MAE of about 5 means the model’s concrete strength predictions are off by around 5 MPa on average. \n",
    "  \n",
    "- In the context of predicting concrete compressive strength (typically measured in the tens of MPa, often ranging from about 15 MPa up to 80+ MPa for certain mixes), having an average error of around 5 MPa is a pretty good result for starting off. \n",
    "\t- Concrete strengths can vary due to factors not always captured in the data (e.g., curing conditions), so even a perfectly measured dataset can have noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation loss over epochs\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Code Explanation:**\n",
    "\n",
    "1. **Import `matplotlib.pyplot`:**\n",
    "    \n",
    "    - Used for visualising the loss curves over the training process.\n",
    "        \n",
    "2. **`plt.plot(history.history['loss'])`:**\n",
    "    \n",
    "    - Plots the training loss (MSE) recorded at each epoch.\n",
    "        \n",
    "3. **`plt.plot(history.history['val_loss'])`:**\n",
    "    \n",
    "    - Plots the validation loss (MSE) at each epoch to compare against training performance.\n",
    "        \n",
    "4. **Labelling and displaying:**\n",
    "    \n",
    "    - Adds a title, axis labels, and legend for clarity.\n",
    "        \n",
    "    - `plt.show()` displays the final plot.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Step?**\n",
    "\n",
    "- Visualising training and validation losses helps identify:\n",
    "    \n",
    "    - Whether the model is learning effectively.\n",
    "        \n",
    "    - If and when overfitting or underfitting occurs.\n",
    "        \n",
    "    - Whether the model converges and if further epochs would help.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### **Result:**\n",
    "\n",
    "- The **blue line** (training loss) starts at approximately **1580 MSE**, and the **orange line** (validation loss) starts at around **1490 MSE**.\n",
    "    \n",
    "- Both lines drop rapidly over the first **15 epochs**, with the training loss settling just above **200 MSE** and the validation loss just below **200 MSE**.\n",
    "    \n",
    "- After around epoch **15**, both curves flatten out and continue a slow, steady convergence downward.\n",
    "    \n",
    "- By epoch **100**, both losses converge to around **75 MSE**, showing:\n",
    "    \n",
    "    - Good learning progress.\n",
    "        \n",
    "    - No significant overfitting (both lines move in parallel with similar values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "R² Score: 0.8429\n",
      "Model Accuracy (R² as percentage): 84.29%\n"
     ]
    }
   ],
   "source": [
    "# Import r2_score from sklearn\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the R² score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Convert R² to percentage accuracy\n",
    "accuracy_percentage = r2 * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Model Accuracy (R² as percentage): {accuracy_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Code Explanation:**\n",
    "\n",
    "- **`from sklearn.metrics import r2_score`:**\n",
    "    \n",
    "    - Imports the `r2_score` function, which measures how well predictions match actual values in regression problems.\n",
    "        \n",
    "- **`y_pred = model.predict(X_test_scaled)`:**\n",
    "    \n",
    "    - Uses the trained model to predict concrete strength values on the scaled test dataset.\n",
    "        \n",
    "- **`r2 = r2_score(y_test, y_pred)`:**\n",
    "    \n",
    "    - Calculates the R² score, showing how well the predicted values explain the variability in the actual results.\n",
    "        \n",
    "    - An R² of 1 means perfect predictions; 0 means no predictive power.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Step?**\n",
    "\n",
    "- While MAE and MSE tell us _how far off_ predictions are on average, they do not show _how well_ the model explains overall variation in the data.\n",
    "    \n",
    "- **R² score** is widely used in regression tasks to indicate model performance as a percentage-based accuracy measure.\n",
    "    \n",
    "- This helps express performance in a way that’s intuitive (closer to the classification idea of accuracy out of 100%).\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### **Result**\n",
    "\n",
    "- An **R² score of 0.8429** means that **84.29% of the variance in concrete compressive strength** is explained by the model’s predictions.\n",
    "    \n",
    "- Overall, an R² score over **80%** is a solid indicator that the model is performing well for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Sources**\n",
    "\n",
    "- **[UCI Machine Learning Repository — Concrete Compressive Strength Dataset](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength)**\n",
    "    \n",
    "    - The dataset contains **1,030 observations** with **9 attributes**: 8 input features and 1 output variable.\n",
    "        \n",
    "    - Input features include amounts (in kg/m³) of cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, fine aggregate, and the age of the concrete (in days).\n",
    "        \n",
    "    - The output variable is the **compressive strength of concrete (MPa)**.\n",
    "        \n",
    "    - The dataset is unscaled and does not contain missing values, making it ideal for modelling after basic preprocessing.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## **Pre-Processing**\n",
    "\n",
    "- **Loading the data**:\n",
    "    \n",
    "    - Initially provided as an `.xls` file, it was converted to `.csv` for easier handling with `pandas`.\n",
    "        \n",
    "- **Renaming Columns**:\n",
    "    \n",
    "    - Columns were renamed to shorter, more convenient names (e.g., `Blast Furnace Slag` → `Blast_Furnace_Slag`).\n",
    "        \n",
    "- **Splitting the dataset**:\n",
    "    \n",
    "    - Separated features (X) and target variable (y).\n",
    "        \n",
    "    - Split into **80% training data** and **20% testing data** using `train_test_split`.\n",
    "        \n",
    "- **Scaling**:\n",
    "    \n",
    "    - Applied `StandardScaler` to normalise feature values.\n",
    "        \n",
    "    - This step was crucial for neural networks, ensuring that larger-valued features don’t dominate weight adjustments during training.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## **Data Visualisation and Understanding**\n",
    "\n",
    "- **Loss Curves**:\n",
    "    \n",
    "    - Plotted both training and validation loss (MSE) across 100 epochs.\n",
    "        \n",
    "    - The blue line (training loss) and orange line (validation loss) both started high (~1580 and ~1490, respectively), quickly dropped, and converged to around **75 MSE** after 100 epochs.\n",
    "        \n",
    "    - This steady and parallel drop indicated stable learning and good generalisation without overfitting.\n",
    "        \n",
    "- **Metric Inspection**:\n",
    "    \n",
    "    - Final validation MAE stabilised around **5 MPa**, indicating relatively tight prediction variance.\n",
    "        \n",
    "    - R² score of **84.29%** confirmed that the model explains the majority of variance in compressive strength.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## **Algorithms**\n",
    "\n",
    "- **Multi-Layer Perceptron (MLP)**\n",
    "    \n",
    "    - A supervised neural network with fully connected layers (`Dense`) was used.\n",
    "        \n",
    "    - Chosen because MLPs are well-suited for tabular regression problems and can model non-linear relationships between input features and output.\n",
    "        \n",
    "- **Key concepts referenced**:\n",
    "    \n",
    "    - **Sequential Model** (Keras): Linear stacking of layers.\n",
    "        \n",
    "    - **Dense Layers**: Fully connected layers to extract feature combinations.\n",
    "        \n",
    "    - **Activation Function (ReLU)**: Chosen for hidden layers to avoid vanishing gradient problems and ensure efficient training.\n",
    "        \n",
    "    - **Loss Function (MSE)**: Used to penalise larger errors more strongly in regression.\n",
    "        \n",
    "    - **Optimizer (Adam)**: Selected for its adaptive learning rate and robust convergence.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## **Model Training and Evaluation**\n",
    "\n",
    "- **Training Setup**:\n",
    "    \n",
    "    - Trained for **100 epochs** using **80% of the dataset** with a **20% validation split**.\n",
    "        \n",
    "    - Tracked `loss` and `val_loss` (MSE), and `mae` metrics across epochs.\n",
    "        \n",
    "- **Performance Results**:\n",
    "    \n",
    "    - **Final test MAE**: ~5.10 MPa.\n",
    "        \n",
    "    - **Test loss (MSE)**: 40.41.\n",
    "        \n",
    "    - **R² Score**: 0.8429, translating to a model accuracy of **84.29%**.\n",
    "        \n",
    "- **Interpretation**:\n",
    "    \n",
    "    - The model consistently improved and showed stable convergence without divergence or overfitting, as both training and validation curves flattened together.\n",
    "        \n",
    "    - A 5 MPa error margin is considered acceptable given the material’s natural variability and measurement conditions.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## **Online Resources and Sources**\n",
    "\n",
    "- **[Scikit-Learn Documentation - Supervised Neural Networks (MLPRegressor)](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)**\n",
    "    \n",
    "- **[GeeksForGeeks — Multi-Layer Perceptron Using Sklearn](https://www.geeksforgeeks.org/multi-layer-perceptron-a-supervised-neural-network-model-using-sklearn/)**\n",
    "    \n",
    "- **[Machine Learning Mastery — First Neural Network with Keras](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/)**\n",
    "    \n",
    "- **[Vanishing and Exploding Gradients (GeeksForGeeks)](https://www.geeksforgeeks.org/vanishing-and-exploding-gradients-problems-in-deep-learning/)**\n",
    "    \n",
    "    - This resource was key in understanding why ReLU is typically used.\n",
    "        \n",
    "- **[Concrete Compressive Strength Dataset - UCI](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength)**\n",
    "    \n",
    "    - Official dataset source.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## **Tools and Technologies**\n",
    "\n",
    "- **Python 3** — Programming language used throughout the project.\n",
    "    \n",
    "- **Pandas** — For data loading, preprocessing, and exploration.\n",
    "    \n",
    "- **Scikit-learn** — For train-test splitting, scaling, and metrics calculation.\n",
    "    \n",
    "- **TensorFlow & Keras** — For building, training, and evaluating the neural network.\n",
    "    \n",
    "- **Matplotlib** — For plotting loss curves and visualising training progress.\n",
    "    \n",
    "- **Jupyter Notebook** — Interactive environment for iterative model development.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## **Challenges Faced**\n",
    "\n",
    "1. **Understanding Neural Network Structure**\n",
    "    \n",
    "    - Initially struggled with grasping how layers, neurons, and activations work.\n",
    "        \n",
    "    - Researched resources to better understand the role of each component.\n",
    "        \n",
    "2. **Understanding Optimisers**\n",
    "    \n",
    "    - Needed to learn what an optimiser does and why Adam was chosen over others.\n",
    "        \n",
    "    - Found out that Adam adapts learning rates and converges quickly without manual tuning.\n",
    "        \n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "- A supervised **multi-layer perceptron** neural network was successfully developed and trained to predict **concrete compressive strength** based on material composition and age.\n",
    "    \n",
    "- The model achieved a strong **R² score of 84.29%**, indicating solid predictive capability.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
